""" 
GPT: generative pretrained transformer
chatGPT -> GPT + agent

transformer: Transformers process data in parallel using a mechanism called self-attention,
which allows the model to weigh the importance of different words in a sentence, regardless of their position.

deep learning architecture 'attention is all you need 2017'


ATTENTION IS ALL YOU NEED

1. Input and encoding:

tokenization process -> each words are assign a token value (263)
tiktoken (gpt tokenizer) - https://tiktokenizer.vercel.app/

unique token - vocab size (gpt 200k)
 

Vector embeddings of tokens:
- semantic meanings in 3d space for each word w.r to realtion and meaning

embedding projector - https://projector.tensorflow.org/


2. Positional Encoding:
token are same but meaning is diffn:
- the cat sat on the mat
- the mat sat on the cat 


3. Self Attention mechanism:
commute betn tokens and change embeddings according


4. Nueral networks


5. Output Embedding
 - softmax or temperatue: level of creativity/ randomness
 """